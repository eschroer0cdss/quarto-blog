{
  "hash": "f4a4e2ba08078f423d873375432e6ec5",
  "result": {
    "markdown": "---\ntitle: \"DRAFT Implementation Guidance: RCT Randomization & Outreach\"\nauthor: \"Eric Schroer\"\nformat: \n  html:\n    theme: minty \n    code-tools: true\n    code-fold: true\n    code-link: true\n    highlight-style: github \n    toc: true\n    \ndate: today\ndate-format: medium\n\ntitle-block-banner: true\nengine: knitr \n\ncategories: [analysis, R, RCTs, Causal Inference]\n---\n\n\n## Background\n\nThe following covers how to implement aspects of RCT randomization projects, both pre- and post- demonstration. Guidance includes power analysis, randomization, sensitivity tests, and testing for attrition. Cluster and stratification randomization methods are covered.\n\nThis does not cover statistical concepts in detail nor how to conduct an RCT from start to finish, though this references some resources that do so.\n\n***This version removes any drive folders or SharePoint links, only posting dummy data, dummy code, and publicly available resources***.\n\n## Pre-Intervention\n\n### **Creating an Outreach & Analysis Plan**\n\nRCT outreach projects should at minimum clearly identify the following from the outset:\n\n*Analysis*\n\n1.  Research questions and hypothesis\n\n2.  Study population\n\n3.  Randomization procedure\n\n4.  Outcomes\n\n5.  Power analysis\n\n6.  *Optional:* Sensitivity Analysis &/or exploratory analysis\n\n*Outreach*\n\n1.  Stakeholder engagement plan\n\n2.  Outreach timing and volume\n\n3.  Outreach content (if applicable)\n\nDespite covering power analysis, randomization procedure, and other sensitivity tests, this guidance is not a detailed how-to guide for analysis & outreach plans. For more guidance on analysis & outreach plans see the \\[*removed\\]*.\n\n### Implementing Power Analysis in R\n\nPower analysis is needed in the early stage of your analysis plan to ensure you have an adequately sized study. If your study has a pre-determined sample size power analysis can be used to determine the minimum detectable effect (MDE) needed to reject your null hypothesis.\n\nThis can be straightforward if you have a binary outcome variable and can determine the proportion of your sample with the outcome of interest at baseline or pre-demonstration. If you can't determine the proportion with the outcome of interest at baseline you can use comparable estimates or finding in related literature. See Appendix for more details.\n\n*Estimating MDE (with proportions & fixed sample size)*\n\nThe first step is calculating the basline incidence by joining your sample to the baseline outcome data. For example, if your outcome of interest is participation in a workforce program and your experiment will nudge people to sign up for that program via text, you can join your sample to current program participation rates at baseline.\n\nThe percentage participating at baseline will be used to estimate the MDE given your sample size. Use the baseline percentage as your outcome incidence in your control group, or *p1* in the following *power.prop.test*:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(pwr) \n\npower.prop.test(n = 114000, #sample size per group\n                p1 = .028, # your calculated rate of outcome at baseline\n              # p2 = what you're solving for, difference of this & P1 ==  MDE\n                power = 0.8, #standard spec. to avoid false negative is .8\n                alternative = \"two.sided\")  #two sided test is default\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n     Two-sample comparison of proportions power calculation \n\n              n = 114000\n             p1 = 0.028\n             p2 = 0.02998218\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n```\n:::\n:::\n\n\nYou determine MDE by taking the difference of p2 (computed by *power.prop.test*) & p1 (you provided). In this case, the MDE is roughly .19 pp change.\n\nIf you have multiple research questions, you may need to calculate power for each questions. See the Power Analysis section of the \\[*removed\\]*.\n\nIf you need to rely on the literature to determine the likely prevalence of your outcome of interest at baseline, see the *Appendix*.\n\nLikewise, see the *Appendix* for more information on how to use an expected effect size to estimate the needed sample size - this is useful if your sample size is not predetermined and you have reason to believe your experiment will produce a similar effect to comparable literature.\n\nFor conceptual guidance, see J-PAL's guidance, [particularly the definitions and Table 1](https://www.povertyactionlab.org/resource/power-calculations). On [Github](https://github.com/J-PAL/Sample_Size_and_Power) J-PAL also has R and STATA functions to calculate minimum sample size needed given a Minimum Detectable Effect using simulation.\n\n### **Implementing Randomization Procedures in R**\n\nOnce you have your sample size and your finalized experimental design, you can implement randomization. If your experiment involved randomizing SNAP participating adults to receive outreach nudging enrollment in enhanced work supports, your design might be compatible with simple randomization:\n\n\n::: {.cell}\n\n:::\n\n\n-   **Simple Randomization:** Once you've generated your sample dataframe with one row per observation you are randomizing--in this case adults, there are several ways to implement a simple randomization but *always set your seed for reproducibility!*\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Simple Randomization\nN = nrow(df)\n\n\nset.seed(15)        #s etting seed for reproducibility \n\ndf$demonstration <- sample(c(0, 1), # 0 as control, 1 as demonstration \n                           size = N, # number of samples to take\n                           prob = c(.5, .5), # probability for each group\n                           replace = T) # sampling from c(0, 1) with replacement\n```\n:::\n\n\n-   **Cluster Randomization:** Effects can 'spillover' and one way to account for this is to randomize at a cluster-level. You may also be providing an intervention that could have an impact on the cluster and not just individual observation.\n\n    -   For example, you might think that telling one parent, caretaker or adult about the enhanced SNAP work support could increase the likelihood other adults in the family enroll. In that case you may want to randomize at the case level to eliminate within-case spillover effects.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Case/Clustered Randomization\n\nset.seed(15)        #setting seed for reproducibility \n\ndf <- df %>% \n  group_by(case_id) %>% \n  mutate(demonstration = sample(c(0, 1),  #0 as control, 1 as demonstration group\n                                1,  #number of items to choose for the entire case_id group\n                           c(.5, .5), #probability for each group\n                           replace = T)) %>%  #sampling from c(0, 1) with replacement \nungroup()\n```\n:::\n\n\n-   **Blocked Randomization in Stratified Sample:** Stratifying randomizes subgroups separately to ensure equal representation across demonstration and control among each subgroup.\n\n    -   Primary language is an example of a stratifying variable you could consider, especially for smaller sub-groups with a primary language other than English or Spanish. Here the package [*randomizr*](https://declaredesign.org/r/randomizr/) is helpful. It can be used to stratify at the case level too.\n\n\n::: {.cell warnings='false'}\n\n```{.r .cell-code}\n# Stratified Case/Clustered Randomization\n\n#install.packages(\"randomizr\")\n#library(randomizr)\n\nset.seed(15)        #setting seed for reproducibility \n\ndf <- df %>% \n  group_by(case_id) %>% \n  mutate(\n    cluster_size = n(), #total adults per case\n    strata = language # if stratifying on mult vars this could be paste0(var1, \"_\", language/var2)\n)\n\ndf$demonstration <- randomizr::block_and_cluster_ra(clusters = df$case_id,\n                                         blocks = df$strata, #your stratification or block variable\n                                         prob_each = c(.5, .5), # adjust as needed for treatment balance\n                                         conditions = c(\"demonstration\", \"control\") #assigned value\n)\n```\n:::\n\n\n-   **Multiple Treatment Arms:** Your outreach experiment may also include multiple versions of the message content to test which communication messages are most effective. If that was the case, you could easily adapt the block and cluster randomization arguments as follows:\n\n\n::: {.cell warnings='false'}\n\n```{.r .cell-code}\n#Two Versions of Outreach Messages\nset.seed(15)        #setting seed for reproducibility \n\ndf <- df %>% \n  group_by(case_id) %>% \n  mutate(\n    cluster_size = n(), \n    strata = language \n)\n\ndf$demonstration <- randomizr::block_and_cluster_ra(clusters = df$case_id,\n                                         blocks = df$strata,\n                                         prob_each = c(.5/2, #message group 1\n                                                       .5/2, #message group 2\n                                                       .5), # adjust as needed for treatment balance\n                                         conditions = c(\"message_vers_one\", #assigned value 1\n                                                        \"message_vers_two\", #assigned value 2\n                                                        \"control\")          #assigned value 3\n)\n```\n:::\n\n\n-   **Phase-in** or **Step-wedge**: If you are providing the demonstration to *everyone* but staggering the timing by which the control group receives the demonstration or treatment, you would follow the exact *block_and_cluster_ra* function as above.\n\n    -   See \\[*removed\\]* for an example of multiple treatment arm phased in *block_and_cluster_ra* script\n\n    -   See \\[*removed\\]* on causal inference for more background on this design.\n\n### Balance Testing\n\nAfter randomizing the data but pre-intervention, it is essential to balance test across key variables. This ensures they are equally represented across demonstration groups.\n\nFor instance, if by chance you have more people already participating in the enhanced work supports in your control group at baseline, this could impact the validity of the analysis and might warrant setting a new seed and randomizing again until outcomes and control variables are more equally represented across groups.\n\n#### nnet & stargazer\n\nThis can be done with two packages in R, *nnet* to create a linear regression model and *stargazer* to present the information and a table. The end of the below code chunk also includes commented function to save the stargazer results for later review:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#install.packages(\"nnet\", \"stargazer\")\nlibrary(nnet)\nlibrary(stargazer)\n\n#Creating Sample + Baseline Outcome Dataset\nsample_baseline_df <- df %>% \n  left_join(outcome_df,\n            by = \"case_id\") %>% \n  group_by(case_id) %>% \n  slice(1) # selecting one row per case because sample is clustered\n\noutcome_language_balance <- multinom(\n    demonstration ~ baseline_occurrence + language, \n    data = sample_baseline_df,\n    trace = FALSE ) # preventing multinom from printing weights \n\n\n# stargazer_language_balance <- stargazer(outcome_language_balance,\n#                                       type = \"text\")  #commented out to suppress printing\n\n\n#Saving File with Seed Number in Title\n#write.table(outcome_language_balance, \n#            paste0(\"stargazer outputs/seed 15_\",\n#            \"Block_Cluster_Balance_Test.txt\"))\n\n# Returning Stargazer Output with more labels & formatting\n\nstargazer(outcome_language_balance,  \n          type = \"text\",\n          title = \"Baseline Testing of Demonstration Groups\",\n          style = \"aer\", # you can set styles of output to resemble academic journal\n          omit.stat = \"aic\",  # argument for ommitting certain statistic codes\n          covariate.labels = c( # covariate/row labels\n                       \"Baseline Occurrence\",\n                       \"Language: Mandarin\",\n                       \"Language: Russian\",\n                       \"Language: Spanish\",\n                       \"Constant\"),\n          notes = \"Comparison group is message version one, language: English\" #notes at bottom\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nBaseline Testing of Demonstration Groups\n===============================================================================\n                            message_vers_two                   control         \n                                   (1)                           (2)           \n-------------------------------------------------------------------------------\nBaseline Occurrence               0.279                         0.160          \n                                 (0.203)                       (0.176)         \n                                                                               \nLanguage: Mandarin               -0.069                        -0.035          \n                                 (0.390)                       (0.334)         \n                                                                               \nLanguage: Russian                 0.003                        -0.086          \n                                 (0.597)                       (0.524)         \n                                                                               \nLanguage: Spanish                 0.029                         0.021          \n                                 (0.215)                       (0.186)         \n                                                                               \nConstant                         -0.148                       0.611***         \n                                 (0.179)                       (0.153)         \n                                                                               \n-------------------------------------------------------------------------------\nNotes:              ***Significant at the 1 percent level.                     \n                    **Significant at the 5 percent level.                      \n                    *Significant at the 10 percent level.                      \n                    Comparison group is message version one, language: English \n```\n:::\n:::\n\n\nCoefficients are on top and standard errors are below in parenthesis. The statistically significant *constant* in the control group simply reflects how the control group is twice as large as the other two groups; this is because our the demonstration group is by design split into a message version one and message version two at 25% of the total sample.\n\nIn our dummy data, there are no statistically significant differences in observable characteristics between the groups.\n\n## **Post-Intervention**\n\n### Measuring Attrition\n\nSample observations may 'drop out' before you're able to measure outcomes of interest. One way this could happen is if a portion of your sample is on your caseload at the time of sampling and even messaging, but exits the program by the time you measure outcomes.\n\nAttrition can impact the study validity or statistical power. It may also reflect an underlying bias on observed or unobserved variables where certain sub-groups systematically drop off more frequently than others. Consequently, it's important to check for *overall attrition* and *differential attrition.*\n\nTo continue with the example of messaging for enhanced work supports for CalFresh families, attrition would be measured by the proportion of families still on the caseload before the completion of the outcome measurement period. The package *kableExtra* can be used for cleaner summary tables:\n\n\n::: {.cell tbl-cap='Overall Attrition Rates'}\n\n```{.r .cell-code}\n#install.packages(\"kableExtra\")\n#library(kableExtra)\n\nN_families = n_distinct(df$case_id)\n\n# creating attrition table by treatment\ndf %>% \n  group_by(active_case) %>% \n  summarize(`N families` = n_distinct(case_id),\n            `% of total sample` = round((`N families`/N_families)*100, 1),\n            .groups = 'keep') %>% \n  mutate(active_case = ifelse(active_case == 1, 'Yes', 'No')) %>% \n  rename(`Active Case`= active_case) %>% \n  kbl %>% \n  kable_minimal(position='center',full_width=F, fixed_thead=T, bootstrap_options='hover')\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\" lightable-minimal\" style='font-family: \"Trebuchet MS\", verdana, sans-serif; width: auto !important; margin-left: auto; margin-right: auto;'>\n <thead>\n  <tr>\n   <th style=\"text-align:left;position: sticky; top:0; background-color: #FFFFFF;\"> Active Case </th>\n   <th style=\"text-align:right;position: sticky; top:0; background-color: #FFFFFF;\"> N families </th>\n   <th style=\"text-align:right;position: sticky; top:0; background-color: #FFFFFF;\"> % of total sample </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> No </td>\n   <td style=\"text-align:right;\"> 69 </td>\n   <td style=\"text-align:right;\"> 8.8 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Yes </td>\n   <td style=\"text-align:right;\"> 713 </td>\n   <td style=\"text-align:right;\"> 91.2 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\nWhen looking at differential attrition, use an OLS multivariate regression or ordinary least-squares model to determine whether any particular variable has a meaningful relationship with attrition. The package *modelsummary* can be used for cleaner presentation of OLS models:\n\n\n::: {.cell tbl-cap='Differential Attrition by Treatment & Language'}\n\n```{.r .cell-code}\n#install.packages(\"modelsummary\")\n#library(modelsummary)\n\n# Only looking at case level\ndf_cases <- df %>% \n  distinct(case_id, .keep_all = TRUE)\n\n# Creating Model\nattrit_model = lm(active_case ~ demonstration + language, data = df_cases)\n\n# Presenting Model using Model Summary\nmodelsummary(list('Active Case (pp)'=attrit_model),\n             stars=T, # significance stars\n             fmt = function(x)round(x*100,2), # formatting both coefficients & standard errors to appear as percents\n             coef_map = c(\n               \"demonstrationmessage_vers_two\" = \"Message Version Two\",\n               \"demonstrationcontrol\" = \"Control\",\n               \"languageMandarin\" = \"Language: Mandarin\",\n               \"languageRussian\" = \"Language: Russian\", \n               \"languageSpanish\" = \"Language: Spanish\",\n               \"(Intercept)\" = \"Intercept\"),\n             statistic = 'Std.Error: ({std.error})',\n             output = 'kableExtra',\n             gof_omit = 'AIC|BIC|F|Log.Lik.|Std.Errors') %>% \n  footnote(general = 'Reference group is demonstration version one, language: english. Coefficients and std.errors multiplied by 100 and presented as percents') %>% \n  kable_minimal() %>% \n  row_spec(seq(1,12,2), bold=T, background = '#ffffbf')\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table style='NAborder-bottom: 0; width: auto !important; margin-left: auto; margin-right: auto;border-bottom: 0; font-family: \"Trebuchet MS\", verdana, sans-serif; margin-left: auto; margin-right: auto;' class=\"table lightable-minimal\">\n <thead>\n  <tr>\n   <th style=\"text-align:left;\">   </th>\n   <th style=\"text-align:center;\"> Active Case (pp) </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;font-weight: bold;background-color: #ffffbf !important;\"> Message Version Two </td>\n   <td style=\"text-align:center;font-weight: bold;background-color: #ffffbf !important;\"> −2.58 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\">  </td>\n   <td style=\"text-align:center;\"> Std.Error: (2.87) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;font-weight: bold;background-color: #ffffbf !important;\"> Control </td>\n   <td style=\"text-align:center;font-weight: bold;background-color: #ffffbf !important;\"> −3.06 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\">  </td>\n   <td style=\"text-align:center;\"> Std.Error: (2.49) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;font-weight: bold;background-color: #ffffbf !important;\"> Language: Mandarin </td>\n   <td style=\"text-align:center;font-weight: bold;background-color: #ffffbf !important;\"> 2.09 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\">  </td>\n   <td style=\"text-align:center;\"> Std.Error: (3.91) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;font-weight: bold;background-color: #ffffbf !important;\"> Language: Russian </td>\n   <td style=\"text-align:center;font-weight: bold;background-color: #ffffbf !important;\"> 4.18 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\">  </td>\n   <td style=\"text-align:center;\"> Std.Error: (6.11) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;font-weight: bold;background-color: #ffffbf !important;\"> Language: Spanish </td>\n   <td style=\"text-align:center;font-weight: bold;background-color: #ffffbf !important;\"> −1.28 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\">  </td>\n   <td style=\"text-align:center;\"> Std.Error: (2.15) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;font-weight: bold;background-color: #ffffbf !important;\"> Intercept </td>\n   <td style=\"text-align:center;font-weight: bold;background-color: #ffffbf !important;\"> 93.61*** </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;box-shadow: 0px 1.5px\">  </td>\n   <td style=\"text-align:center;box-shadow: 0px 1.5px\"> Std.Error: (2.3) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Num.Obs. </td>\n   <td style=\"text-align:center;\"> 782 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> R2 </td>\n   <td style=\"text-align:center;\"> 0.004 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> R2 Adj. </td>\n   <td style=\"text-align:center;\"> −0.003 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> RMSE </td>\n   <td style=\"text-align:center;\"> 0.28 </td>\n  </tr>\n</tbody>\n<tfoot><tr><td style=\"padding: 0; \" colspan=\"100%\">\n<sup></sup> + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001</td></tr></tfoot>\n<tfoot>\n<tr><td style=\"padding: 0; \" colspan=\"100%\"><span style=\"font-style: italic;\">Note: </span></td></tr>\n<tr><td style=\"padding: 0; \" colspan=\"100%\">\n<sup></sup> Reference group is demonstration version one, language: english. Coefficients and std.errors multiplied by 100 and presented as percents</td></tr>\n</tfoot>\n</table>\n\n`````\n:::\n:::\n\n\nIn our dummy data, there are no differential rates of attrition. The experiment would be ready to carry out the analysis related to essential research questions.\n\n## Appendix\n\n### RCTs: Very Brief Review of Concepts\n\n#### Counterfactuals & Causality\n\nThe power of an Randomized Control Trial (RCT) is that in theory systematic differences across the demonstration and control groups, whether observed in administrative data or unknown, will be equally represented across groups at a given time. The results for your demonstration group would not fully generalize to your control group.\n\nFor more resources:\n\n-   \\[*removed\\]* for a brief overview of counterfactuals, causality, and selection bias\n\n-   While focused on observational data & matching, the '[Crash course in potential outcomes](despite%20the%20post%20focus%20on%20observational%20matching)' section in [Andrew Heiss' post](https://www.andrewheiss.com/blog/2024/03/21/demystifying-ate-att-atu/#ref-MorganWinship:2014) covers individual & average effects and selection bias.\n\n-   J-PAL has more extensive resources on the entire project lifecycle for [randomized evaluations](https://www.povertyactionlab.org/resource/introduction-randomized-evaluations)\n\n#### 'Nudging' Through Outreach\n\nRCTs where the demonstration group is texted or emailed to secure their accounts or apply for a program are nudges because they present a simple, predictable choice without introducing any new incentive - the benefits of securing their account or enrolling in CalFresh were already available to them.\n\nFor more resources:\n\n-   [A meta-analysis](https://onlinelibrary.wiley.com/doi/abs/10.3982/ECTA18709) of RCT nudges experiments across public agency produced and academic literature, and a comparison between these two types of research\n\n-   The federal [Office of Evaluation Sciences](https://oes.gsa.gov/work/) includes analysis plan registrations and results for cross disciplinary research. Methods used can include RCTs with or without outreach nudges and quasi-experimental methodologies.\n\n### Power Analysis: Other Methods\n\n*Estimating Sample Size*\n\nIf using a power analysis to estimate how large your sample should be, the main variable you need to estimate is *Cohen's d*, or the mean difference between groups divided by pooled standard deviation.\n\nTwo options for this are relying on pilots or published literature. When it comes to nudge RCTs, we have both.\n\n-   *Relying on Literature*\n\n    -   [RCTs to Scale: Comprehensive Evidence From Two Nudge Units (wiley.com)](https://onlinelibrary.wiley.com/doi/epdf/10.3982/ECTA18709) conducts a meta-analysis of over 120 governmental 'nudge' RCTs, finding an unweighted impact of 0.014 (1.4%), a standard error of 0.3, and an average control group take-up of 17.3%.\n\n-   *In House Pilot Evaluations*\n\n    -   The mean impact of messages on PIN Change via the EBT PIN change experiment was .01 (1%), pooled standard deviation was .107, and the rate of PIN change in the control group was .0027 (.27%).\n        -   Depending on your experiment, you may want to use the mean impact and deviation of messages on theft.\n\nUsing this information and the *pwr* library, we could calculate sample size needed as follows:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#| label: power-calc-est-sample\n#| warning: false\n\nlibrary(pwr) \n\npower.prop.test(#n = 114000, #solving for this\n                p1 = 0.173, # literature avg. control take-up\n                p2 = 0.187, # control + lit avg. impact \n                power = 0.8, #standard spec. to avoid false negative is .8\n                alternative = \"two.sided\")  #two sided test is default\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n     Two-sample comparison of proportions power calculation \n\n              n = 11820.19\n             p1 = 0.173\n             p2 = 0.187\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n```\n:::\n:::\n\n\nThe above example uses the meta analyses figures and determines you need at least 11,821 observations in *each group*. If you wanted to use your pilot study or did not have a reliable estimate from the literature, you would change p1 and p2 accordingly.\n\nFor conceptual guidance, see J-PAL's guidance, [particularly the definitions and Table 1](https://www.povertyactionlab.org/resource/power-calculations). On [Github](https://github.com/J-PAL/Sample_Size_and_Power) J-PAL also has R and STATA functions to calculate minimum sample size needed given a Minimum Detectable Effect using simulation.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"../../site_libs/kePrint-0.0.1/kePrint.js\"></script>\r\n<link href=\"../../site_libs/lightable-0.0.1/lightable.css\" rel=\"stylesheet\" />\r\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}